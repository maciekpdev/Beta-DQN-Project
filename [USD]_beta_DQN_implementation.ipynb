{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maciekpdev/Beta-DQN-Project/blob/google-colab/%5BUSD%5D_beta_DQN_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lJ5139XYyk4M",
        "outputId": "ff10bf83-cba0-4151-bde2-8924369e7a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.6.1)\n",
            "Collecting minigrid\n",
            "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from minigrid) (1.2.2)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
            "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minigrid\n",
            "Successfully installed minigrid-3.0.0\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/dist-packages (from ale-py) (2.0.2)\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.12/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (2.0.2)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.12/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.1.0)\n",
            "Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp312-cp312-linux_x86_64.whl size=535721 sha256=9944c537c08ad5094fabb68d78310024d512ea4e00feccb203e5aac639bfd9c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/a7/fa/9b0357f258d2e68bdc71df972e02418bceb02355ac1f365c59\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n",
            "Collecting pympler\n",
            "  Downloading Pympler-1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading Pympler-1.1-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.8/165.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pympler\n",
            "Successfully installed pympler-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[classic-control]\n",
        "!pip install minigrid\n",
        "!pip install ale-py\n",
        "!pip install gym-super-mario-bros\n",
        "!pip install pympler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import deque\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "AFND5nc8_plo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN"
      ],
      "metadata": {
        "id": "2C54Y3L4_zfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, state_shape, dtype=np.uint8):\n",
        "        self.capacity = capacity\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "\n",
        "        self.states = np.zeros((capacity, *state_shape), dtype=dtype)\n",
        "        self.next_states = np.zeros((capacity, *state_shape), dtype=dtype)\n",
        "        self.actions = np.zeros(capacity, dtype=np.int64)\n",
        "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
        "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
        "\n",
        "    def push(self, s, a, r, s2, done):\n",
        "        self.states[self.pos] = s\n",
        "        self.next_states[self.pos] = s2\n",
        "        self.actions[self.pos] = a\n",
        "        self.rewards[self.pos] = r\n",
        "        self.dones[self.pos] = done\n",
        "\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "        self.full = self.full or self.pos == 0\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_idx = self.capacity if self.full else self.pos\n",
        "        idxs = np.random.randint(0, max_idx, size=batch_size)\n",
        "\n",
        "        return (self.states[idxs],\n",
        "                self.actions[idxs],\n",
        "                self.rewards[idxs],\n",
        "                self.next_states[idxs],\n",
        "                self.dones[idxs])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.pos\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, obs_shape, num_actions):\n",
        "        \"\"\"\n",
        "        Deep Q-Network (DQN) architecture based on:\n",
        "\n",
        "        Young, K.,  Tian, T. (2019).\n",
        "        \"MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible\n",
        "        Reinforcement Learning Experiments.\" https://arxiv.org/abs/1903.03176\n",
        "\n",
        "        obs_shape: (C, H, W)\n",
        "        num_actions: int\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.is_image = len(obs_shape) == 3\n",
        "\n",
        "        if self.is_image:\n",
        "            c, h, w = obs_shape\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=c, out_channels=16, kernel_size=3, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Flatten()\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.zeros(1, c, h, w)\n",
        "                output_size = self.net(dummy).shape[1]\n",
        "\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(output_size, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, num_actions)\n",
        "            )\n",
        "        else:\n",
        "            input_dim = obs_shape[0]\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 128),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            self.fc = nn.Linear(128, num_actions)\n",
        "    def forward(self, x):\n",
        "        if self.is_image and x.max() > 1.0: x = x / 255.0\n",
        "        x = self.net(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class BenchDQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, config, device):\n",
        "        self.device = device\n",
        "        self.action_dim = action_dim\n",
        "        self.config = config # Store config\n",
        "\n",
        "        # Read Hyperparameters from config\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.epsilon = config[\"epsilon_start\"]\n",
        "        self.epsilon_min = config[\"epsilon_min\"]\n",
        "        self.epsilon_decay = config[\"epsilon_decay\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.lr = config[\"lr\"]\n",
        "\n",
        "        # Init Networks\n",
        "        self.policy_net = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
        "\n",
        "        # Init Memory\n",
        "        self.memory = ReplayBuffer(config[\"buffer_size\"], state_dim)\n",
        "\n",
        "    def select_action(self, state, policy, training=True):\n",
        "        if training and np.random.rand() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "\n",
        "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            return self.policy_net(state_t).argmax().item()\n",
        "\n",
        "    def store_transition(self, s, a, r, ns, d):\n",
        "        self.memory.push(s, a, r, ns, d)\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions = torch.LongTensor(np.array(actions)).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(np.array(rewards)).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
        "        dones = torch.FloatTensor(np.array(dones)).to(self.device)\n",
        "\n",
        "        curr_q = self.policy_net(states).gather(1, actions).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_states).max(1)[0]\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = F.smooth_l1_loss(curr_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fuNnmvqj11SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NoisyNet-DQN"
      ],
      "metadata": {
        "id": "CQKr4Yjw_1FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "jY0uBQ9gKJP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "beta-DQN"
      ],
      "metadata": {
        "id": "v2YEEYKMKg7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BetaNetwork(DQN):\n",
        "    def forward(self, x):\n",
        "        logits = super().forward(x)\n",
        "        return torch.softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "IRWo9awMVVaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "class CovPolicy:\n",
        "    def __init__(self, delta):\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, Q_values, beta_probs, epsilon):\n",
        "        actions = torch.argsort(Q_values)\n",
        "\n",
        "        if all(beta_probs[a] > self.delta for a in actions):\n",
        "            return max(actions, key=lambda a: Q_values[a]).item(), False\n",
        "\n",
        "        low_coverage_actions = [a for a in actions if beta_probs[a] <= self.delta]\n",
        "        return random.choice(low_coverage_actions).item(), True\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CovPolicy delta={self.delta}\"\n",
        "\n",
        "class CorPolicy:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, Q, beta, epsilon):\n",
        "        actions = torch.argsort(Q)\n",
        "        min_q = torch.min(Q)\n",
        "\n",
        "        action = max(\n",
        "            actions,\n",
        "            key=lambda a: self.alpha * Q[a]\n",
        "            + (1 - self.alpha) * (Q[a] if beta[a] > epsilon else min_q)\n",
        "        )\n",
        "\n",
        "        return action.item(), self.is_exploration(action, actions, Q, beta, epsilon)\n",
        "\n",
        "    def is_exploration(self, action, actions, Q, beta, epsilon):\n",
        "        return action != max(actions, key=lambda a: Q[a] if beta[a] > epsilon else 0)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CorPolicy alpha={self.alpha}\"\n"
      ],
      "metadata": {
        "id": "q5RX9IYAYDtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "class MetaController:\n",
        "    def __init__(self, window_size=1000):\n",
        "        cor_policies = [CorPolicy(1/10) for i in range(1, 10)]\n",
        "        cov_policies = [CovPolicy(0.05), CovPolicy(0.1)]\n",
        "        self.policies = cor_policies + cov_policies # Zbiór polityk użytych w artykule\n",
        "\n",
        "        self.num_policies = len(self.policies)\n",
        "        self.window_size = window_size\n",
        "        self.history = deque(maxlen=window_size)  # automatycznie usuwa ostatnie\n",
        "        # Przechowuje (policy_idx, reward, exploration_ratio)\n",
        "\n",
        "    def select_policy(self):\n",
        "        used_indices = [h[0] for h in self.history]\n",
        "        for i in range(self.num_policies):\n",
        "            if i not in used_indices:\n",
        "                return self.policies[i]\n",
        "\n",
        "        best_value = -float(\"inf\")\n",
        "        best_policy = 0\n",
        "\n",
        "        for i in range(self.num_policies):\n",
        "            value = self.count_mean(i) + self.count_exploration_bonus(i)\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_policy = i\n",
        "\n",
        "        return self.policies[best_policy]\n",
        "\n",
        "\n",
        "    def update(self, policy_idx, reward, exploration_ratio):\n",
        "        self.history.append((policy_idx, reward, exploration_ratio))\n",
        "\n",
        "    def count_mean(self, i):\n",
        "        rewards = [reward for policy_idx, reward, _ in self.history if policy_idx == i]\n",
        "        if not rewards:\n",
        "            return 0.0\n",
        "        return sum(rewards) / len(rewards)\n",
        "\n",
        "    def count_exploration_bonus(self, i): # bk(pi, L)\n",
        "        n = self.count_policy_occurance(i)\n",
        "        if n == 0:\n",
        "            return float(\"inf\")\n",
        "        return (1 / n) * self.count_sum_of_exploration_ratio(i)\n",
        "\n",
        "\n",
        "    def count_sum_of_exploration_ratio(self, i): # E Bm(pii)\n",
        "        return sum(exploration_ratio for policy_idx, _, exploration_ratio in self.history if policy_idx == i)\n",
        "\n",
        "    def count_policy_occurance(self, i): # Nk(pi, L)\n",
        "        return sum(1 for policy_idx, _, _ in self.history if policy_idx == i)"
      ],
      "metadata": {
        "id": "qlDuSMnMZbtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BetaDQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, config, device):\n",
        "        self.device = device\n",
        "        self.action_dim = action_dim\n",
        "        self.config = config # Store config\n",
        "\n",
        "        # Read Hyperparameters from config\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.epsilon = config[\"epsilon_start\"]\n",
        "        self.epsilon_min = config[\"epsilon_min\"]\n",
        "        self.epsilon_decay = config[\"epsilon_decay\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.lr = config[\"lr\"]\n",
        "\n",
        "        # Init Networks\n",
        "        self.policy_net = DQN(state_dim, action_dim).to(device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(device)\n",
        "\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer_q = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
        "\n",
        "        self.beta_net = DQN(state_dim, action_dim).to(device)\n",
        "        self.optimizer_beta = torch.optim.Adam(self.beta_net.parameters(), lr=self.lr)\n",
        "\n",
        "        # Init Memory\n",
        "        self.memory = ReplayBuffer(config[\"buffer_size\"], state_dim)\n",
        "\n",
        "    def select_action(self, state, policy, training=True):\n",
        "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "\n",
        "        if training:\n",
        "           with torch.no_grad():\n",
        "            qvals = self.policy_net(state_t)\n",
        "            beta = self.beta_net(state_t)\n",
        "\n",
        "           return policy(qvals, beta, self.epsilon)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            return self.policy_net(state_t).argmax().item()\n",
        "\n",
        "    def store_transition(self, s, a, r, ns, d):\n",
        "        self.memory.push(s, a, r, ns, d)\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions = torch.LongTensor(np.array(actions)).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(np.array(rewards)).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
        "        dones = torch.FloatTensor(np.array(dones)).to(self.device)\n",
        "\n",
        "        loss = self.train_td(states, actions, rewards, next_states, dones)\n",
        "        self.train_beta_net(states, actions)\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train_td(self, states, actions, rewards, next_states, dones):\n",
        "        curr_q = self.policy_net(states).gather(1, actions).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_states).max(1)[0]\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = F.smooth_l1_loss(curr_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train_beta_net(self, states, actions):\n",
        "        logits = self.beta_net(states)\n",
        "        loss = F.cross_entropy(logits, actions)\n",
        "        self.optimizer_beta.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer_beta.step()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n"
      ],
      "metadata": {
        "id": "QR2TKSfHKkJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from minigrid.wrappers import ImgObsWrapper\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "import ale_py\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
        "\n",
        "def make_env(env_name, render_mode=None):\n",
        "    \"\"\"\n",
        "    Creates the environment with necessary wrappers based on the name.\n",
        "    \"\"\"\n",
        "    if env_name == \"MountainCar-v0\":\n",
        "        return gym.make(\"MountainCar-v0\", render_mode=render_mode)\n",
        "\n",
        "    elif \"LavaCrossing\" in env_name:\n",
        "        # Example: MiniGrid-LavaCrossingS9N1-v0\n",
        "        return gym.make(env_name, render_mode=render_mode)\n",
        "        return env\n",
        "\n",
        "    elif env_name == \"ALE/Breakout-v5\":\n",
        "        gym.register_envs(ale_py)\n",
        "        env = gym.make(env_name, frameskip=1, render_mode=render_mode)\n",
        "        env = AtariPreprocessing(env, grayscale_obs=True, scale_obs=False)\n",
        "        env = FrameStackObservation(env, stack_size=4)\n",
        "        return env\n",
        "\n",
        "    elif \"SuperMarioBros\" in env_name:\n",
        "        env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "        env = JoypadSpace(env, RIGHT_ONLY)\n",
        "        return env\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown environment: {env_name}\")"
      ],
      "metadata": {
        "id": "pLtY6BBo20Mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7f1df3-8d2d-4e31-848d-9f5cd5f967be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(agent_class, env_name, algorithm_name, config, seeds=[42], save_dir=\"results\", meta_controller=None):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    all_seeds_history = {}\n",
        "\n",
        "    max_steps = config[\"total_steps\"]\n",
        "\n",
        "    print(f\"\\n--- Starting: {algorithm_name} on {env_name} ---\")\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\" > Seed {seed}...\")\n",
        "        env = make_env(env_name)\n",
        "        obs, _ = env.reset(seed=seed)\n",
        "\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Init Agent\n",
        "        state_dim = env.observation_space.shape\n",
        "        action_dim = env.action_space.n\n",
        "        agent = agent_class(state_dim, action_dim, config, device=device)\n",
        "\n",
        "        history = {\"episode_rewards\": [], \"global_steps\": [], \"loss\": []}\n",
        "        global_step_count = 0\n",
        "        episode_count = 0\n",
        "\n",
        "        # Track Best Performance\n",
        "        best_avg_reward = -float('inf')\n",
        "\n",
        "        while global_step_count < max_steps:\n",
        "            state, _ = env.reset()\n",
        "            episode_count += 1\n",
        "            total_reward = 0\n",
        "            total_exploration_steps = 0\n",
        "            ep_losses = []\n",
        "            done = False\n",
        "            truncated = False\n",
        "            policy = None\n",
        "            steps_per_episode = 0\n",
        "\n",
        "            if meta_controller:\n",
        "              policy = meta_controller.select_policy()\n",
        "\n",
        "            while not (done or truncated):\n",
        "                if meta_controller:\n",
        "                  action, is_exploration_move = agent.select_action(state, policy, training=True)\n",
        "                  if is_exploration_move:\n",
        "                    total_exploration_steps += 1\n",
        "                else:\n",
        "                  action = agent.select_action(state, policy, training=True)\n",
        "\n",
        "                next_state, reward, done, truncated, _ = env.step(action)\n",
        "                agent.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "                if global_step_count % config.get(\"train_frequency\", 4) == 0:\n",
        "                    loss = agent.train_step()\n",
        "                    if loss is not None:\n",
        "                        ep_losses.append(loss)\n",
        "\n",
        "                if global_step_count % config.get(\"target_update_freq\", 1000) == 0:\n",
        "                    agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                global_step_count += 1\n",
        "                steps_per_episode +=1\n",
        "\n",
        "                if global_step_count >= max_steps:\n",
        "                    print(f\"   [Limit Reached] Hit max steps at Episode {episode_count}\")\n",
        "                    break\n",
        "\n",
        "            if meta_controller:\n",
        "              meta_controller.update(policy, total_reward, total_exploration_steps / steps_per_episode)\n",
        "\n",
        "            agent.decay_epsilon()\n",
        "\n",
        "            # Logging\n",
        "            avg_loss = np.mean(ep_losses) if ep_losses else 0\n",
        "            history[\"episode_rewards\"].append(total_reward)\n",
        "            history[\"global_steps\"].append(global_step_count)\n",
        "            history[\"loss\"].append(avg_loss)\n",
        "\n",
        "            avg_r = np.mean(history[\"episode_rewards\"][-50:])\n",
        "\n",
        "            if episode_count > 10000 and avg_r > best_avg_reward:\n",
        "                  best_avg_reward = avg_r\n",
        "                  save_path = f\"{save_dir}/{algorithm_name}_{env_name}_seed{seed}_best.pth\"\n",
        "                  torch.save(agent.policy_net.state_dict(), save_path)\n",
        "                  # print(f\"   [Saved Best] New Record: {best_avg_reward:.2f}\")\n",
        "\n",
        "            if episode_count % 50 == 0:\n",
        "                print(f\"   Step {global_step_count}/{max_steps} (Ep {episode_count}) | Avg Reward: {avg_r:.2f} | Loss: {avg_loss:.4f} | Eps: {agent.epsilon:.2f}\")\n",
        "\n",
        "        # 3. Save Final Model at end of training\n",
        "        final_path = f\"{save_dir}/{algorithm_name}_{env_name}_seed{seed}_final.pth\"\n",
        "        torch.save(agent.policy_net.state_dict(), final_path)\n",
        "        print(f\"   [Saved Final] Saved to {final_path}\")\n",
        "\n",
        "        all_seeds_history[seed] = history\n",
        "        env.close()\n",
        "\n",
        "    # Save Data\n",
        "    data = {\"algorithm\": algorithm_name, \"env\": env_name, \"seeds_data\": all_seeds_history}\n",
        "    with open(f\"{save_dir}/{algorithm_name}_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "mYogFDv53SgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(data):\n",
        "    algo_name = data[\"algorithm\"]\n",
        "    plot_data = []\n",
        "    for seed, history in data[\"seeds_data\"].items():\n",
        "        for i, steps in enumerate(history[\"global_steps\"]):\n",
        "            plot_data.append({\n",
        "                \"Algorithm\": algo_name,\n",
        "                \"Global Steps\": steps,\n",
        "                \"Reward\": history[\"episode_rewards\"][i]\n",
        "            })\n",
        "    df = pd.DataFrame(plot_data)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df, x=\"Global Steps\", y=\"Reward\", errorbar='sd')\n",
        "    plt.title(f\"{algo_name} Training Curve\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zhiK5kGf6moI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e76a3a9"
      },
      "source": [
        "MC_CONFIG = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"gamma\": 0.99,\n",
        "    \"batch_size\": 32,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"epsilon_start\": 1.0,\n",
        "    \"epsilon_min\": 0.05,\n",
        "    \"epsilon_decay\": 0.95,  # Decay every episode\n",
        "    \"target_update_freq\": 10, # Update target net every 10 episodes (optional logic)\n",
        "    \"train_frequency\": 4,\n",
        "    \"total_steps\": 5_000_000\n",
        "}\n",
        "\n",
        "BREAKOUT_CONFIG = { #FROM PAPER\n",
        "    \"lr\": 1e-3,\n",
        "    \"gamma\": 0.99,\n",
        "    \"batch_size\": 32,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"epsilon_start\": 1.0,\n",
        "    \"epsilon_min\": 0.01,\n",
        "    \"epsilon_decay\": 0.999,\n",
        "    \"target_update_freq\": 1000, # Update target net every 10 episodes (optional logic)\n",
        "    \"train_frequency\": 4,\n",
        "    \"total_steps\": 5_000_000\n",
        "}\n",
        "\n",
        "seeds=[31]#, 12, 1123, 111, 145]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "breakout_data = run_benchmark(\n",
        "    agent_class=BetaDQNAgent,\n",
        "    env_name=\"ALE/Breakout-v5\",\n",
        "    algorithm_name=\"BetaDQN\",\n",
        "    config=BREAKOUT_CONFIG,\n",
        "    seeds=seeds,\n",
        "    save_dir=\"results_breakout\",\n",
        "    meta_controller=MetaController()\n",
        ")\n",
        "\n",
        "plot_results(breakout_data)"
      ],
      "metadata": {
        "id": "fk6pMS6dQRXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "collapsed": true,
        "outputId": "68862986-08f8-4571-ce35-d241491d37cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "\n",
            "--- Starting: BetaDQN on ALE/Breakout-v5 ---\n",
            " > Seed 31...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4131120842.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running on: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m breakout_data = run_benchmark(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0magent_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBetaDQNAgent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ALE/Breakout-v5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1091537117.py\u001b[0m in \u001b[0;36mrun_benchmark\u001b[0;34m(agent_class, env_name, algorithm_name, config, seeds, save_dir, meta_controller)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmeta_controller\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                   \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_exploration_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0mis_exploration_move\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mtotal_exploration_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3851295947.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, policy, training)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3098003132.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, Q, beta, epsilon)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmin_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         action = max(\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3098003132.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     28\u001b[0m         action = max(\n\u001b[1;32m     29\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmin_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         )\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    }
  ]
}